{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24aa918b-9785-436a-9346-178740c3cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END2END RESNET \n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0889fc5c-1ed1-4f0c-90ec-3acc1f670fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_INTERVAL = 5  # Capture every 5th frame\n",
    "CLIP_LENGTH = 16  # Number of frames per clip for 3D CNN\n",
    "FRAME_HEIGHT, FRAME_WIDTH = 112, 112  # r3d_18 with input 112x112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37220eab-22a6-479d-800b-844802d73919",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\"\n",
    "SAVE_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\Processed_Frames\"\n",
    "Anomaly_dir = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\anomaly_annotation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b4f4bb-8d85-4ce7-ae42-7b76709af172",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalising the features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215b6d6-0fba-4b17-b500-17c3ed7905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Anomaly_data = pd.read_csv(Anomaly_dir)\n",
    "anomalies = set([anon.split(\"_\")[0] for anon in Anomaly_data.name.values])\n",
    "print(f'Anomalies: {anomalies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c055e9cb-3ddd-4944-a9a2-c10c339ce363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory for extracted frames\n",
    "\n",
    "def extract_and_save_frames(video_path, save_dir, frame_interval=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    save_folder = os.path.join(save_dir, video_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR (OpenCV) to RGB (PIL)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Apply transformations\n",
    "            frame = transform(frame)  # Now it's a Tensor (C, H, W)\n",
    "\n",
    "            # Convert back to PIL image to save\n",
    "            frame = transforms.ToPILImage()(frame)\n",
    "\n",
    "            # Save frame as JPEG\n",
    "            frame_path = os.path.join(save_folder, f\"frame_{saved_count:04d}.jpg\")\n",
    "            frame.save(frame_path, \"JPEG\")\n",
    "            saved_count += 1 \n",
    "\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918ee46-93a8-4f43-a866-6c9d4595d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Frames: 0it [00:00, ?it/s]\n",
      "Extracting Frames: 0it [00:00, ?it/s]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:16<00:00,  9.13s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:22<00:00,  5.47s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 13/13 [02:32<00:00, 11.69s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 25/25 [05:13<00:00, 12.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 22/22 [02:31<00:00,  6.87s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 43/43 [03:07<00:00,  4.36s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 29/29 [04:04<00:00,  8.42s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 17/17 [01:39<00:00,  5.88s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 39/39 [03:33<00:00,  5.47s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 12/12 [01:54<00:00,  9.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:30<00:00,  9.07s/it]\n",
      "Extracting Frames:  48%|█████████████████████████████████████████                                            | 58/120 [16:54<11:11, 10.84s/it]"
     ]
    }
   ],
   "source": [
    "for root, _, files in os.walk(VIDEO_DIR):\n",
    "    for video_file in tqdm(files, desc=f\"Extracting Frames {root}\"):\n",
    "        if video_file.endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "            video_path = os.path.join(root, video_file)\n",
    "            anon_label = extract_and_save_frames(video_path, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375e646-5a7a-495e-9eb3-1ce828fa3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, labels_df, transform=None, clip_length=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.clip_length = clip_length\n",
    "        labels_df  # Columns: [name\tscenario\ttotal frames\tstarting frame of anomaly\tending frame of anomaly]\n",
    "        all_anomalies = \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data[\"label\"] = self.label_encoder.fit_transform(self.data[\"label\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name, label = self.data.iloc[idx]\n",
    "        video_folder = os.path.join(self.root_dir, video_name)\n",
    "        frame_files = sorted(os.listdir(video_folder))  # Ensure chronological order\n",
    "        \n",
    "        # Select 16 frames (random or sequential)\n",
    "        start_idx = np.random.randint(0, max(1, len(frame_files) - self.clip_length))\n",
    "        selected_frames = frame_files[start_idx:start_idx + self.clip_length]\n",
    "        \n",
    "        frames = [self.transform(cv2.imread(os.path.join(video_folder, f))) for f in selected_frames]\n",
    "        frames = torch.stack(frames)  # Shape: (16, 3, 112, 112) \n",
    "\n",
    "        return frames, torch.tensor(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
