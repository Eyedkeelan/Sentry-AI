{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea6ac8b6-0d59-458e-8a27-8998021bc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END2END RESNET \n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800919cf-1114-498f-97af-cb141b5e8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_INTERVAL = 5  # Capture every 5th frame\n",
    "CLIP_LENGTH = 16  # Number of frames per clip for 3D CNN\n",
    "FRAME_HEIGHT, FRAME_WIDTH = 224, 224  # r3d_18 with input 112x112 Slowfast 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded568fa-5734-4df6-8b88-568bfd87abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\"\n",
    "SAVE_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\Processed_Frames\"\n",
    "Anomaly_dir = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\anomaly_annotation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dc273bb-4e46-46ed-aced-3a5ce30187be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalising the features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d85cdf18-899d-4bad-a756-6f9c9c022b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies: {'Traffic', 'Water', 'Object', 'Fire', 'People', 'Shooting', 'Vandalism', 'Fighting', 'Robbery', 'Explosion', 'Assault'}\n"
     ]
    }
   ],
   "source": [
    "Anomaly_data = pd.read_csv(Anomaly_dir)\n",
    "anomalies = set([anon.split(\"_\")[0] for anon in Anomaly_data.name.values])\n",
    "print(f'Anomalies: {anomalies}')\n",
    "\n",
    "anno_names = Anomaly_data.name.values.tolist()\n",
    "anno_start = Anomaly_data['starting frame of anomaly'].values.tolist()\n",
    "anno_end = Anomaly_data['ending frame of anomaly'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca535937-6d07-43e0-ab25-9cd93b8fe012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory for extracted frames\n",
    "\n",
    "def extract_and_save_frames(video_path, save_dir, frame_interval=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    save_folder = os.path.join(save_dir, video_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR (OpenCV) to RGB (PIL)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Apply transformations\n",
    "            frame = transform(frame)  # Now it's a Tensor (C, H, W)\n",
    "\n",
    "            # Convert back to PIL image to save\n",
    "            frame = transforms.ToPILImage()(frame)\n",
    "\n",
    "            # Save frame as JPEG\n",
    "            frame_path = os.path.join(save_folder, f\"frame_{saved_count:04d}.jpg\")\n",
    "            frame.save(frame_path, \"JPEG\")\n",
    "            saved_count += 1 \n",
    "\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2169e237-6fe1-4d1c-a871-d31e7f19942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur: 0it [00:00, ?it/s]\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur: 0it [00:00, ?it/s]\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Ass\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Exp\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Fig\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Fir\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Obj\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Peo\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Rob\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Sho\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Tra\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Van\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_anomaly_blur\\Wat\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_testing_b\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_videos_blur: 0it [00:00, ?it/s]\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n",
      "Extracting Frames C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\\MSAD_normal_training_\n"
     ]
    }
   ],
   "source": [
    "# My iteration through\n",
    "for root, _, files in os.walk(VIDEO_DIR):\n",
    "    for video_file in tqdm(files, desc=f\"Extracting Frames {root}\"):\n",
    "        if video_file.endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "            video_path = os.path.join(root, video_file)\n",
    "            extract_and_save_frames(video_path, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba43863d-f98f-41b3-9191-93a4d40a7024",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = []\n",
    "anonamly_bool = []\n",
    "frame_paths = []\n",
    "frames = []\n",
    "video_names = []\n",
    "video_path = []\n",
    "import os\n",
    "for root, _, files in os.walk(SAVE_DIR):\n",
    "   for name in files:\n",
    "      frame_path = os.path.join(root, name)\n",
    "      components = frame_path.split(os.sep) \n",
    "      video_name =  components[-2]\n",
    "      frame = int(components[-1].split(\"_\")[1].split(\".\")[0]) * FRAME_INTERVAL\n",
    "      frames.append(frame)\n",
    "      video_names.append(video_name)\n",
    "      #print(video_name) \n",
    "      frame_paths.append(frame_path)\n",
    "      anom = video_name.split(\"_\")[0]\n",
    "      if anom in anomalies:\n",
    "          #print(frame,video_name)\n",
    "          pos = anno_names.index(video_name)\n",
    "          start = anno_start[pos]\n",
    "          end = anno_end[pos]\n",
    "\n",
    "          if start < frame < end: \n",
    "              anon_bool = 1 \n",
    "              anomaly_label = anom\n",
    "          else:\n",
    "              anon_bool = 0\n",
    "              anomaly_label = \"Normal\"\n",
    "         \n",
    "      else:\n",
    "          anon_bool = 0\n",
    "          anomaly_label = \"Normal\"\n",
    "      anomaly.append(anomaly_label)\n",
    "      anonamly_bool.append(anon_bool)\n",
    "\n",
    "\n",
    "metadata = pd.DataFrame({'Video':video_names,\n",
    "              'Frame':frames,\n",
    "             'Frames_path':frame_paths, \n",
    "             \"Anomaly Type\": anomaly,\n",
    "             \"Anomaly\": anonamly_bool})\n",
    "\n",
    "metadata[\"Video\"] = metadata[\"Video\"].str.replace(\"MSAD_normal_\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48eb94a2-700f-4aac-87ed-3c5d69a44b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assault_3</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assault_5</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Assault_6</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assault_9</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>testing_116</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>testing_117</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>testing_118</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>testing_119</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>testing_120</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Video partition\n",
       "0      Assault_1     Train\n",
       "1      Assault_3     Train\n",
       "2      Assault_5     Train\n",
       "3      Assault_6     Train\n",
       "4      Assault_9     Train\n",
       "..           ...       ...\n",
       "235  testing_116      Test\n",
       "236  testing_117      Test\n",
       "237  testing_118      Test\n",
       "238  testing_119      Test\n",
       "239  testing_120      Test\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_I3D_WS_Train.list') as train:\n",
    "    t = train.readlines()\n",
    "    train_list = [item.split(\"\\n\")[0].split(\"/\")[-1].replace(\"_i3d.npy\",\"\") for item in t]\n",
    "    train_label = [\"Train\"] * len(train_list)\n",
    "tr_labels = pd.DataFrame({\"Video\":train_list,\n",
    "                        \"partition\":train_label}) \n",
    "with open(r'C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_I3D_WS_Test.list') as test:\n",
    "    t = test.readlines()\n",
    "    test_list = [item.split(\"\\n\")[0].split(\"/\")[-1].replace(\"_i3d.npy\",\"\") for item in t]\n",
    "    test_label = [\"Test\"] * len(test_list)\n",
    "\n",
    "te_labels = pd.DataFrame({\"Video\":test_list,\n",
    "                         \"partition\":test_label})\n",
    "label_df = pd.concat([tr_labels,te_labels])\n",
    "label_df[\"Video\"] = label_df[\"Video\"].str.replace(\"MSAD_normal_\", \"\", regex=False)\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a5a2775-0653-49ae-a88e-c55f21710afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left= metadata, right = label_df , on= \"Video\",how= \"left\")\n",
    "df_train =  df[df[\"partition\"] == \"Train\"].drop(columns= \"partition\")\n",
    "df_test =  df[df[\"partition\"] == \"Test\"].drop(columns= \"partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c769c60d-6815-4813-8392-0964c155bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, labels_df, transform=None, clip_length=32, tau=4, step_size=8):\n",
    "        \"\"\"\n",
    "        labels_df: DataFrame with columns [\"Video\", \"Frame\", \"Frames_path\", \"Anomaly Type\", \"Anomaly\"]\n",
    "        transform: Image transformations (for resizing, normalizing, etc.)\n",
    "        clip_length: Number of frames per sample for Fast Path (default 32)\n",
    "        tau: Frame stride for Slow Path (default 4, meaning every 4th frame)\n",
    "        step_size: How far the window moves per sample (default 8)\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df.copy()  # Prevent modifying the original DataFrame\n",
    "        self.transform = transform\n",
    "        self.clip_length = clip_length\n",
    "        self.tau = tau\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Ensure data is sorted by video and frame number\n",
    "        self.labels_df.sort_values(by=[\"Video\", \"Frame\"], inplace=True)\n",
    "\n",
    "        # Group frames by video\n",
    "        self.video_groups = self.labels_df.groupby(\"Video\")\n",
    "\n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels_df[\"Encoded_Label\"] = self.label_encoder.fit_transform(self.labels_df[\"Anomaly\"])\n",
    "\n",
    "        # Store unique videos\n",
    "        self.video_list = list(self.video_groups.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        total_clips = 0\n",
    "        for video_name in self.video_list:\n",
    "            num_frames = len(self.video_groups.get_group(video_name))\n",
    "            num_clips = max(0, (num_frames - self.clip_length) // self.step_size + 1)\n",
    "            total_clips += num_clips\n",
    "        return total_clips\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine video & clip index\n",
    "        total_clips = 0\n",
    "        for video_name in self.video_list:\n",
    "            video_frames = self.video_groups.get_group(video_name)\n",
    "            num_frames = len(video_frames)\n",
    "            num_clips = max(0, (num_frames - self.clip_length) // self.step_size + 1)\n",
    "\n",
    "            if idx < total_clips + num_clips:\n",
    "                clip_idx = idx - total_clips\n",
    "                break\n",
    "\n",
    "            total_clips += num_clips\n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx} is out of range for dataset of length {self.__len__()}\")\n",
    "\n",
    "        # Get frames & labels\n",
    "        frame_paths = video_frames[\"Frames_path\"].tolist()\n",
    "        frame_labels = video_frames[\"Encoded_Label\"].tolist()\n",
    "\n",
    "        start_idx = clip_idx * self.step_size\n",
    "\n",
    "        # Select frames for **Fast Path**\n",
    "        fast_frames_paths = frame_paths[start_idx:start_idx + self.clip_length]\n",
    "        fast_labels = frame_labels[start_idx:start_idx + self.clip_length]\n",
    "\n",
    "        # Select frames for **Slow Path** (every `tau` frames from fast frames)\n",
    "        slow_frames_paths = fast_frames_paths[::self.tau]\n",
    "        slow_labels = fast_labels[::self.tau]\n",
    "\n",
    "        # Ensure Slow Path has enough frames\n",
    "        while len(slow_frames_paths) < self.clip_length // self.tau:\n",
    "            slow_frames_paths.append(slow_frames_paths[-1])\n",
    "            slow_labels.append(slow_labels[-1])\n",
    "\n",
    "        # Load images\n",
    "        def load_frames(paths):\n",
    "            frames = []\n",
    "            for frame_path in paths:\n",
    "                frame = cv2.imread(frame_path)\n",
    "                if frame is None:\n",
    "                    raise ValueError(f\"Error reading image: {frame_path}\")\n",
    "\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                else:\n",
    "                    frame = torch.from_numpy(frame).float()\n",
    "\n",
    "                frames.append(frame)\n",
    "\n",
    "            return torch.stack(frames)  # Shape: (T, C, H, W)\n",
    "\n",
    "        fast_frames = load_frames(fast_frames_paths)\n",
    "        slow_frames = load_frames(slow_frames_paths)\n",
    "\n",
    "        # Transpose to match SlowFast input format (C, T, H, W)\n",
    "        fast_frames = fast_frames.permute(1, 0, 2, 3)  # (C, T, H, W)\n",
    "        slow_frames = slow_frames.permute(1, 0, 2, 3)  # (C, T/4, H, W)\n",
    "\n",
    "        # Assign majority label\n",
    "        clip_label = torch.tensor(fast_labels).mode()[0]  # Get most frequent label\n",
    "\n",
    "        return [slow_frames, fast_frames], torch.tensor(clip_label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c8df11e-c93e-4c13-a027-8183d192bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 5598\n",
      "testing dataset size: 2814\n"
     ]
    }
   ],
   "source": [
    "training_dataset = FrameDataset(labels_df = df_train)\n",
    "testing_dataset = FrameDataset(labels_df = df_test)\n",
    "print(\"Training dataset size: {}\\ntesting dataset size: {}\".format(len(training_dataset),len(testing_dataset)))\n",
    "num_workers = min(6, os.cpu_count() - 1)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=16, shuffle=True, num_workers=0)##, pin_memory=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=16, shuffle=True, num_workers=0)#, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e89f7af-bd0f-4fbc-931a-7ff68e850f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Keelan.Butler/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "slowfast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "\n",
    "# Remove classification head\n",
    "slowfast_model.blocks[-1] = nn.Identity() # Remove the final classification layer\n",
    "\n",
    "class SlowFastBinaryClassifier(nn.Module):\n",
    "    def __init__(self, slowfast_model):\n",
    "        super(SlowFastBinaryClassifier, self).__init__()\n",
    "        self.slowfast_model = slowfast_model\n",
    "        self.fc = nn.Linear(2304 , 1)  # Output 1 for binary classification\n",
    "\n",
    "    def forward(self, slow_frames, fast_frames):\n",
    "        features = self.slowfast_model([slow_frames, fast_frames])  # Forward pass\n",
    "        #print(\"Feature shape before pooling:\", features.shape)\n",
    "        \n",
    "        features = F.adaptive_avg_pool3d(features, (1, 1, 1)).squeeze()\n",
    "\n",
    "        return self.fc(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaf5245b-6182-49cc-8bec-e3d18b6e1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = SlowFastBinaryClassifier(slowfast_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ffa91-af57-42c7-82c8-d1fc24fe6f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training pass:   0%|                                                                                                   | 0/350 [00:00<?, ?it/s]C:\\Users\\Keelan.Butler\\AppData\\Local\\Temp\\ipykernel_15332\\541321580.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return [slow_frames, fast_frames], torch.tensor(clip_label, dtype=torch.long)\n",
      "Training pass: 100%|█████████████████████████████████████████████████████████████████████████████████████| 350/350 [13:43:30<00:00, 141.17s/it]\n",
      "Cycling Testing Dataloader: 100%|██████████████████████████████████████████████████████████████████████████| 176/176 [1:03:15<00:00, 21.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Optimal model: 1 epoch\n",
      "Epoch [1/25] - Training Loss: 0.1616, Test Loss: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training pass: 100%|█████████████████████████████████████████████████████████████████████████████████████| 350/350 [14:10:20<00:00, 145.77s/it]\n",
      "Cycling Testing Dataloader: 100%|████████████████████████████████████████████████████████████████████████████| 176/176 [51:34<00:00, 17.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25] - Training Loss: 0.0513, Test Loss: 0.5424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training pass: 100%|█████████████████████████████████████████████████████████████████████████████████████| 350/350 [15:18:28<00:00, 157.45s/it]\n",
      "Cycling Testing Dataloader: 100%|████████████████████████████████████████████████████████████████████████████| 176/176 [53:41<00:00, 18.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25] - Training Loss: 0.0325, Test Loss: 0.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training pass:  38%|████████████████████████████████▏                                                   | 134/350 [5:12:22<8:21:02, 139.18s/it]"
     ]
    }
   ],
   "source": [
    "# Training Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_binary = SlowFastBinaryClassifier(slowfast_model)\n",
    "\n",
    "epochs = 25\n",
    "losses = np.zeros((2, epochs))\n",
    "model_binary.to(device)\n",
    "\n",
    "optimiser = optim.Adam(model_binary.parameters(), lr=1e-4, weight_decay=1e-4)  \n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use GPU if available\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "threshhold = 0.5 \n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model_binary.train()\n",
    "\n",
    "    for frames, labels in tqdm(training_dataloader, desc=f\"Training pass epoch: {epoch}\"):\n",
    "        torch.cuda.empty_cache()  \n",
    "        slow_frames, fast_frames = frames  # Unpack SlowFast inputs\n",
    "        \n",
    "        slow_frames, fast_frames, labels = (\n",
    "            slow_frames.to(device),\n",
    "            fast_frames.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        slow_frames = slow_frames.permute(0, 4, 2, 3, 1)\n",
    "        fast_frames = fast_frames.permute(0, 4, 2, 3, 1)\n",
    "        \n",
    "        \n",
    "        #print(\"slow_frames shape:\", slow_frames.shape)\n",
    "        #print(\"fast_frames shape:\", fast_frames.shape)\n",
    "\n",
    "        pred = model_binary(slow_frames, fast_frames)\n",
    "        loss = loss_function(pred.squeeze(), labels.float())\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        torch.cuda.empty_cache()  \n",
    "    # Store training loss\n",
    "    losses[0, epoch] = epoch_loss / len(training_dataloader)\n",
    "\n",
    "    # Validation Loop\n",
    "    model_binary.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_frames, test_labels in tqdm(testing_dataloader, desc=\"Cycling Testing Dataloader\"):\n",
    "            slow_test_frames, fast_test_frames = test_frames  # Unpack test data\n",
    "            \n",
    "            slow_test_frames, fast_test_frames, test_labels = (\n",
    "                slow_test_frames.to(device),\n",
    "                fast_test_frames.to(device),\n",
    "                test_labels.to(device),\n",
    "            )\n",
    "            slow_test_frames = slow_test_frames.permute(0, 4, 2, 3, 1)\n",
    "            fast_test_frames = fast_test_frames.permute(0, 4, 2, 3, 1)\n",
    "\n",
    "            test_preds = model_binary(slow_test_frames, fast_test_frames)  \n",
    "            t_loss = loss_function(test_preds.squeeze(), test_labels.float())\n",
    "\n",
    "            test_loss += t_loss.item()\n",
    "            \n",
    "    losses[1, epoch] = test_loss / len(testing_dataloader)\n",
    "\n",
    "    # Save best model\n",
    "    if best_loss > losses[1, epoch]:\n",
    "        best_loss = losses[1, epoch] \n",
    "        print(f\"Saving Optimal model: {epoch + 1} epoch\")\n",
    "        torch.save(model_binary.state_dict(), os.path.join(\"Best_Models\", \"E2E_SF.pt\"))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {losses[0,epoch]:.4f}, Test Loss: {losses[1,epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf572668-0cb3-4eaf-8d28-189be1aeb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0], label = 'Training')\n",
    "plt.plot(losses[1], label = 'Testing')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Slow-Fast Binary Classification Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da1319a8-cb86-4a0a-ad58-00268bfca9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    }
   ],
   "source": [
    "model_binary.load_state_dict(torch.load(os.path.join(\"Best_Models\", \"E2E_SF.pt\")))\n",
    "print(\"Best model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13190fd4-0354-47dd-9f38-46f95fe77e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/176 [00:00<?, ?it/s]C:\\Users\\Keelan.Butler\\AppData\\Local\\Temp\\ipykernel_15332\\541321580.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return [slow_frames, fast_frames], torch.tensor(clip_label, dtype=torch.long)\n",
      "  1%|▌                                                                                                         | 1/176 [00:15<45:31, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                        | 2/176 [00:32<46:54, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                                                        | 3/176 [00:53<53:18, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                                       | 4/176 [01:11<52:19, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                                       | 5/176 [01:28<51:11, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▌                                                                                                      | 6/176 [01:46<50:36, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▏                                                                                                     | 7/176 [02:03<49:58, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▊                                                                                                     | 8/176 [02:26<54:02, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▍                                                                                                    | 9/176 [02:47<55:13, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▉                                                                                                   | 10/176 [03:06<53:50, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██████▌                                                                                                  | 11/176 [03:24<52:08, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▏                                                                                                 | 12/176 [03:42<51:14, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▊                                                                                                 | 13/176 [04:00<50:10, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████▎                                                                                                | 14/176 [04:18<50:05, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▉                                                                                                | 15/176 [04:38<50:53, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████▌                                                                                               | 16/176 [04:54<47:52, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▏                                                                                              | 17/176 [05:10<45:44, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▋                                                                                              | 18/176 [05:25<44:05, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███████████▎                                                                                             | 19/176 [05:41<43:12, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███████████▉                                                                                             | 20/176 [06:00<44:30, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▌                                                                                            | 21/176 [06:18<45:26, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████▏                                                                                           | 22/176 [06:35<44:51, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████▋                                                                                           | 23/176 [06:52<43:31, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████████▎                                                                                          | 24/176 [07:08<42:29, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████████▉                                                                                          | 25/176 [07:24<41:51, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████████▌                                                                                         | 26/176 [07:44<44:15, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████████                                                                                         | 27/176 [08:00<42:43, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████████▋                                                                                        | 28/176 [08:17<42:02, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████████▎                                                                                       | 29/176 [08:33<40:51, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████████▉                                                                                       | 30/176 [08:48<39:52, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████▍                                                                                      | 31/176 [09:04<39:04, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████                                                                                      | 32/176 [09:23<40:53, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████▋                                                                                     | 33/176 [09:40<40:20, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████████▎                                                                                    | 34/176 [09:58<40:47, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████▉                                                                                    | 35/176 [10:14<39:28, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▍                                                                                   | 36/176 [10:29<38:33, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████████████████                                                                                   | 37/176 [10:45<37:50, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████████▋                                                                                  | 38/176 [11:03<38:35, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████▎                                                                                 | 39/176 [11:20<38:42, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████████▊                                                                                 | 40/176 [11:38<39:07, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████▍                                                                                | 41/176 [11:59<41:21, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████                                                                                | 42/176 [12:20<42:38, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████████▋                                                                               | 43/176 [12:41<43:31, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████▎                                                                              | 44/176 [13:05<45:52, 20.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████████████████████▊                                                                              | 45/176 [13:25<44:56, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████████████▍                                                                             | 46/176 [13:45<44:05, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████                                                                             | 47/176 [14:04<43:03, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████████████▋                                                                            | 48/176 [14:23<42:29, 19.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████▏                                                                           | 49/176 [14:43<42:01, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████████████▊                                                                           | 50/176 [15:03<41:47, 19.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████▍                                                                          | 51/176 [15:22<40:49, 19.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████                                                                          | 52/176 [15:41<40:02, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████▌                                                                         | 53/176 [15:59<39:04, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████████████▏                                                                        | 54/176 [16:18<38:20, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████████████▊                                                                        | 55/176 [16:36<37:40, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████▍                                                                       | 56/176 [16:55<37:38, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████████████                                                                       | 57/176 [17:14<37:25, 18.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████████▌                                                                      | 58/176 [17:34<37:51, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████▏                                                                     | 59/176 [17:56<38:52, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████▊                                                                     | 60/176 [18:18<39:43, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████████████▍                                                                    | 61/176 [18:38<39:18, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████████████▉                                                                    | 62/176 [18:59<39:22, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████████████▌                                                                   | 63/176 [19:21<39:17, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████▏                                                                  | 64/176 [19:43<39:47, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████▊                                                                  | 65/176 [20:04<39:06, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████▍                                                                 | 66/176 [20:24<38:24, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████▉                                                                 | 67/176 [20:44<37:42, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████████████▌                                                                | 68/176 [21:07<38:04, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|█████████████████████████████████████████▏                                                               | 69/176 [21:28<37:44, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████▊                                                               | 70/176 [21:50<38:08, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████▎                                                              | 71/176 [22:12<37:46, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|██████████████████████████████████████████▉                                                              | 72/176 [22:35<38:24, 22.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████▌                                                             | 73/176 [22:59<38:42, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████▏                                                            | 74/176 [23:25<40:17, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████████████████████████████████▋                                                            | 75/176 [23:50<40:29, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████████████████▎                                                           | 76/176 [24:13<39:41, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████████████████████████████████████████▉                                                           | 77/176 [24:36<38:37, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████▌                                                          | 78/176 [24:58<37:45, 23.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████▏                                                         | 79/176 [25:19<36:06, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████████▋                                                         | 80/176 [25:39<34:40, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████████████████▎                                                        | 81/176 [25:59<33:40, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████▉                                                        | 82/176 [26:19<32:30, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████████████████████▌                                                       | 83/176 [26:38<31:16, 20.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████                                                       | 84/176 [26:59<31:33, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████▋                                                      | 85/176 [27:19<30:54, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████████████████▎                                                     | 86/176 [27:39<30:28, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████████████████▉                                                     | 87/176 [28:00<30:22, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████▌                                                    | 88/176 [28:21<30:03, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████████████████                                                    | 89/176 [28:40<29:24, 20.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████████████████▋                                                   | 90/176 [29:00<28:52, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████████████████▎                                                  | 91/176 [29:24<30:10, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████████████████▉                                                  | 92/176 [29:47<30:37, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████▍                                                 | 93/176 [30:11<30:46, 22.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████████████████████                                                 | 94/176 [30:32<30:12, 22.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████▋                                                | 95/176 [30:55<30:16, 22.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████▎                                               | 96/176 [31:17<29:41, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████▊                                               | 97/176 [31:39<28:53, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████████████▍                                              | 98/176 [32:00<28:27, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████████████████████████████████████████                                              | 99/176 [32:27<29:55, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████████████████                                             | 100/176 [32:53<30:23, 23.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████████████████▋                                            | 101/176 [33:16<29:49, 23.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████▎                                           | 102/176 [33:39<29:07, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████████████████▊                                           | 103/176 [34:03<28:45, 23.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████████▍                                          | 104/176 [34:27<28:35, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████                                          | 105/176 [34:52<28:35, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████▋                                         | 106/176 [35:19<29:06, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████████████████████████████▏                                        | 107/176 [35:44<28:54, 25.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████████████████████████████████████████████▊                                        | 108/176 [36:07<27:33, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████████▍                                       | 109/176 [36:29<26:16, 23.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████████                                       | 110/176 [36:51<25:24, 23.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████████████████████▌                                      | 111/176 [37:13<24:47, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████████████████████▏                                     | 112/176 [37:36<24:19, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████████████████████████████████▊                                     | 113/176 [37:56<23:08, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████▎                                    | 114/176 [38:17<22:21, 21.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████▉                                    | 115/176 [38:37<21:44, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████▌                                   | 116/176 [38:58<21:13, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████████████████████▏                                  | 117/176 [39:20<21:04, 21.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████▋                                  | 118/176 [39:46<22:02, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████▎                                 | 119/176 [40:11<22:16, 23.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████▉                                 | 120/176 [40:34<21:48, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████████████████▌                                | 121/176 [40:56<20:54, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████                                | 122/176 [41:18<20:17, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████▋                               | 123/176 [41:38<19:18, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████▎                              | 124/176 [42:02<19:26, 22.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████████████████████████████▊                              | 125/176 [42:23<18:42, 22.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████████████████████▍                             | 126/176 [42:46<18:33, 22.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████████                             | 127/176 [43:07<18:04, 22.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████▋                            | 128/176 [43:28<17:23, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████▏                           | 129/176 [43:51<17:13, 21.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████████████████████▊                           | 130/176 [44:13<16:52, 22.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████████████████████████▍                          | 131/176 [44:37<17:04, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████                          | 132/176 [44:58<16:19, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████▌                         | 133/176 [45:19<15:29, 21.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████▏                        | 134/176 [45:40<15:05, 21.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████████████████████▊                        | 135/176 [46:02<14:45, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████▎                       | 136/176 [46:23<14:24, 21.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████████████████████▉                       | 137/176 [46:47<14:22, 22.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████████████████████▌                      | 138/176 [47:09<14:04, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████████████████████████████████████████████████████████▏                     | 139/176 [47:32<13:51, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████▋                     | 140/176 [47:55<13:27, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████▎                    | 141/176 [48:17<13:02, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████████████████████████████▉                    | 142/176 [48:38<12:30, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████▌                   | 143/176 [48:59<11:53, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████████████████████████                   | 144/176 [49:21<11:40, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████████████████████████▋                  | 145/176 [49:44<11:22, 22.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████▎                 | 146/176 [50:03<10:38, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████████████████████████████████████████████████████████▊                 | 147/176 [50:24<10:10, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████████████████████████▍                | 148/176 [50:46<09:58, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████                | 149/176 [51:05<09:18, 20.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████▋               | 150/176 [51:23<08:35, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████▏              | 151/176 [51:40<07:58, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████▊              | 152/176 [51:58<07:29, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████▍             | 153/176 [52:16<07:07, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████             | 154/176 [52:35<06:49, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████████████████████▌            | 155/176 [52:53<06:27, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████▏           | 156/176 [53:11<06:09, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████▊           | 157/176 [53:30<05:49, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████▎          | 158/176 [53:47<05:27, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████▉          | 159/176 [54:06<05:10, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████▌         | 160/176 [54:25<04:54, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████▏        | 161/176 [54:44<04:41, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████▋        | 162/176 [55:05<04:29, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████▎       | 163/176 [55:24<04:10, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████▉       | 164/176 [55:44<03:52, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████▌      | 165/176 [56:04<03:35, 19.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████      | 166/176 [56:23<03:16, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████▋     | 167/176 [56:43<02:57, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|███████████████████████████████████████████████████████████████████████████████████████████████████▎    | 168/176 [57:02<02:35, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████▊    | 169/176 [57:22<02:16, 19.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 170/176 [57:42<01:59, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████   | 171/176 [58:01<01:37, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 172/176 [58:19<01:15, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 173/176 [58:36<00:55, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 174/176 [58:53<00:36, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍| 175/176 [59:10<00:17, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([16, 2304, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 176/176 [59:25<00:00, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape before pooling: torch.Size([14, 2304, 1, 1, 1])\n",
      "Total test samples: 2814 (Expected: 29270)\n",
      "Total predictions: 2814 (Expected: 29270)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for frames, labels in tqdm(dataloader):\n",
    "            frames = frames\n",
    "            labels = labels\n",
    "            slow_frames, fast_frames = frames  # Unpack SlowFast inputs\n",
    "        \n",
    "            slow_frames, fast_frames, labels = (\n",
    "                slow_frames.to(device),\n",
    "                fast_frames.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "\n",
    "            slow_frames = slow_frames.permute(0, 4, 2, 3, 1)\n",
    "            fast_frames = fast_frames.permute(0, 4, 2, 3, 1)\n",
    "            outputs = model(slow_frames, fast_frames)  # Forward pass\n",
    "           # _, preds = torch.max(outputs, 1)  # Get predicted class MULTICLASS APPROACH\n",
    "            #preds = (outputs >= 0.5).float() # Binalry class \n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(device)  # Change to \"cuda\" if using GPU\n",
    "true_labels, pred_labels = get_predictions(model_binary, testing_dataloader, device)\n",
    "print(f\"Total test samples: {len(true_labels)} (Expected: 29270)\")\n",
    "print(f\"Total predictions: {len(pred_labels)} (Expected: 29270)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "439542b5-3f22-4351-a06e-46cc22fa0f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(cm , annot = True)\n",
    "plt.title(\"Confusion Matrix of Slow-Fast Classifier\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e694c-afc7-4c31-955a-a60f06c9f7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
