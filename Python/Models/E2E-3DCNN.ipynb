{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24aa918b-9785-436a-9346-178740c3cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END2END RESNET \n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0889fc5c-1ed1-4f0c-90ec-3acc1f670fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_INTERVAL = 5  # Capture every 5th frame\n",
    "CLIP_LENGTH = 16  # Number of frames per clip for 3D CNN\n",
    "FRAME_HEIGHT, FRAME_WIDTH = 112, 112  # r3d_18 with input 112x112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37220eab-22a6-479d-800b-844802d73919",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_blur\"\n",
    "SAVE_DIR = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\Processed_Frames\"\n",
    "Anomaly_dir = r\"C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\anomaly_annotation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75b4f4bb-8d85-4ce7-ae42-7b76709af172",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalising the features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d215b6d6-0fba-4b17-b500-17c3ed7905eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies: {'Fire', 'Vandalism', 'Shooting', 'Water', 'Assault', 'Object', 'People', 'Fighting', 'Traffic', 'Robbery', 'Explosion'}\n"
     ]
    }
   ],
   "source": [
    "Anomaly_data = pd.read_csv(Anomaly_dir)\n",
    "anomalies = set([anon.split(\"_\")[0] for anon in Anomaly_data.name.values])\n",
    "print(f'Anomalies: {anomalies}')\n",
    "\n",
    "anno_names = Anomaly_data.name.values.tolist()\n",
    "anno_start = Anomaly_data['starting frame of anomaly'].values.tolist()\n",
    "anno_end = Anomaly_data['ending frame of anomaly'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c055e9cb-3ddd-4944-a9a2-c10c339ce363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory for extracted frames\n",
    "\n",
    "def extract_and_save_frames(video_path, save_dir, frame_interval=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    save_folder = os.path.join(save_dir, video_name)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            # Convert BGR (OpenCV) to RGB (PIL)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Apply transformations\n",
    "            frame = transform(frame)  # Now it's a Tensor (C, H, W)\n",
    "\n",
    "            # Convert back to PIL image to save\n",
    "            frame = transforms.ToPILImage()(frame)\n",
    "\n",
    "            # Save frame as JPEG\n",
    "            frame_path = os.path.join(save_folder, f\"frame_{saved_count:04d}.jpg\")\n",
    "            frame.save(frame_path, \"JPEG\")\n",
    "            saved_count += 1 \n",
    "\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7918ee46-93a8-4f43-a866-6c9d4595d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Frames: 0it [00:00, ?it/s]\n",
      "Extracting Frames: 0it [00:00, ?it/s]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:16<00:00,  9.13s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:22<00:00,  5.47s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 13/13 [02:32<00:00, 11.69s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 25/25 [05:13<00:00, 12.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 22/22 [02:31<00:00,  6.87s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 43/43 [03:07<00:00,  4.36s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 29/29 [04:04<00:00,  8.42s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 17/17 [01:39<00:00,  5.88s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 39/39 [03:33<00:00,  5.47s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 12/12 [01:54<00:00,  9.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:30<00:00,  9.07s/it]\n",
      "Extracting Frames: 100%|████████████████████████████████████████████████████████████████████████████████████| 120/120 [32:38<00:00, 16.32s/it]\n",
      "Extracting Frames: 0it [00:00, ?it/s]\n",
      "Extracting Frames: 100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [11:44:41<00:00, 1565.98s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:53<00:00,  6.94s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 30/30 [03:56<00:00,  7.87s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 15/15 [03:05<00:00, 12.35s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [04:35<00:00, 11.47s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 27/27 [05:07<00:00, 11.40s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 29/29 [05:33<00:00, 11.49s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 21/21 [04:02<00:00, 11.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 35/35 [07:18<00:00, 12.52s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 26/26 [03:03<00:00,  7.07s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 27/27 [03:35<00:00,  7.98s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 26/26 [03:55<00:00,  9.06s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 25/25 [03:17<00:00,  7.91s/it]\n",
      "Extracting Frames: 100%|██████████████████████████████████████████████████████████████████████████████████████| 23/23 [02:46<00:00,  7.24s/it]\n"
     ]
    }
   ],
   "source": [
    "for root, _, files in os.walk(VIDEO_DIR):\n",
    "    for video_file in tqdm(files, desc=f\"Extracting Frames {root}\"):\n",
    "        if video_file.endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "            video_path = os.path.join(root, video_file)\n",
    "            extract_and_save_frames(video_path, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e6c59f0-4a89-4970-9f06-b70ff3837e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = []\n",
    "anonamly_bool = []\n",
    "frame_paths = []\n",
    "frames = []\n",
    "video_names = []\n",
    "video_path = []\n",
    "import os\n",
    "for root, _, files in os.walk(SAVE_DIR):\n",
    "   for name in files:\n",
    "      frame_path = os.path.join(root, name)\n",
    "      components = frame_path.split(os.sep) \n",
    "      video_name =  components[-2]\n",
    "      frame = int(components[-1].split(\"_\")[1].split(\".\")[0]) * FRAME_INTERVAL\n",
    "      frames.append(frame)\n",
    "      video_names.append(video_name)\n",
    "      #print(video_name) \n",
    "      frame_paths.append(frame_path)\n",
    "      anom = video_name.split(\"_\")[0]\n",
    "      if anom in anomalies:\n",
    "          #print(frame,video_name)\n",
    "          pos = anno_names.index(video_name)\n",
    "          start = anno_start[pos]\n",
    "          end = anno_end[pos]\n",
    "\n",
    "          if start < frame < end: \n",
    "              anon_bool = 1 \n",
    "              anomaly_label = anom\n",
    "          else:\n",
    "              anon_bool = 0\n",
    "              anomaly_label = \"Normal\"\n",
    "         \n",
    "      else:\n",
    "          anon_bool = 0\n",
    "          anomaly_label = \"Normal\"\n",
    "      anomaly.append(anomaly_label)\n",
    "      anonamly_bool.append(anon_bool)\n",
    "\n",
    "\n",
    "metadata = pd.DataFrame({'Video':video_names,\n",
    "              'Frame':frames,\n",
    "             'Frames_path':frame_paths, \n",
    "             \"Anomaly Type\": anomaly,\n",
    "             \"Anomaly\": anonamly_bool})\n",
    "\n",
    "metadata[\"Video\"] = metadata[\"Video\"].str.replace(\"MSAD_normal_\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a77195a-7b93-4dac-8c93-2b895e873e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Frames_path</th>\n",
       "      <th>Anomaly Type</th>\n",
       "      <th>Anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>5</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>15</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>20</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89666</th>\n",
       "      <td>Water_incident_9</td>\n",
       "      <td>405</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89667</th>\n",
       "      <td>Water_incident_9</td>\n",
       "      <td>410</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89668</th>\n",
       "      <td>Water_incident_9</td>\n",
       "      <td>415</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89669</th>\n",
       "      <td>Water_incident_9</td>\n",
       "      <td>420</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89670</th>\n",
       "      <td>Water_incident_9</td>\n",
       "      <td>425</td>\n",
       "      <td>C:\\Users\\Keelan.Butler\\Desktop\\python_projects...</td>\n",
       "      <td>Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89671 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Video  Frame  \\\n",
       "0             Assault_1      0   \n",
       "1             Assault_1      5   \n",
       "2             Assault_1     10   \n",
       "3             Assault_1     15   \n",
       "4             Assault_1     20   \n",
       "...                 ...    ...   \n",
       "89666  Water_incident_9    405   \n",
       "89667  Water_incident_9    410   \n",
       "89668  Water_incident_9    415   \n",
       "89669  Water_incident_9    420   \n",
       "89670  Water_incident_9    425   \n",
       "\n",
       "                                             Frames_path Anomaly Type  Anomaly  \n",
       "0      C:\\Users\\Keelan.Butler\\Desktop\\python_projects...       Normal        0  \n",
       "1      C:\\Users\\Keelan.Butler\\Desktop\\python_projects...       Normal        0  \n",
       "2      C:\\Users\\Keelan.Butler\\Desktop\\python_projects...       Normal        0  \n",
       "3      C:\\Users\\Keelan.Butler\\Desktop\\python_projects...       Normal        0  \n",
       "4      C:\\Users\\Keelan.Butler\\Desktop\\python_projects...       Normal        0  \n",
       "...                                                  ...          ...      ...  \n",
       "89666  C:\\Users\\Keelan.Butler\\Desktop\\python_projects...        Water        1  \n",
       "89667  C:\\Users\\Keelan.Butler\\Desktop\\python_projects...        Water        1  \n",
       "89668  C:\\Users\\Keelan.Butler\\Desktop\\python_projects...        Water        1  \n",
       "89669  C:\\Users\\Keelan.Butler\\Desktop\\python_projects...        Water        1  \n",
       "89670  C:\\Users\\Keelan.Butler\\Desktop\\python_projects...        Water        1  \n",
       "\n",
       "[89671 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18baf978-0689-4d25-ae34-52256684cf27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assault_1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assault_3</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assault_5</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Assault_6</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assault_9</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>testing_116</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>testing_117</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>testing_118</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>testing_119</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>testing_120</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Video partition\n",
       "0      Assault_1     Train\n",
       "1      Assault_3     Train\n",
       "2      Assault_5     Train\n",
       "3      Assault_6     Train\n",
       "4      Assault_9     Train\n",
       "..           ...       ...\n",
       "235  testing_116      Test\n",
       "236  testing_117      Test\n",
       "237  testing_118      Test\n",
       "238  testing_119      Test\n",
       "239  testing_120      Test\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_I3D_WS_Train.list') as train:\n",
    "    t = train.readlines()\n",
    "    train_list = [item.split(\"\\n\")[0].split(\"/\")[-1].replace(\"_i3d.npy\",\"\") for item in t]\n",
    "    train_label = [\"Train\"] * len(train_list)\n",
    "tr_labels = pd.DataFrame({\"Video\":train_list,\n",
    "                        \"partition\":train_label}) \n",
    "with open(r'C:\\Users\\Keelan.Butler\\Desktop\\python_projects\\Final Project\\OneDrive_2025-01-30\\MSAD Dataset\\MSAD_I3D_WS_Test.list') as test:\n",
    "    t = test.readlines()\n",
    "    test_list = [item.split(\"\\n\")[0].split(\"/\")[-1].replace(\"_i3d.npy\",\"\") for item in t]\n",
    "    test_label = [\"Test\"] * len(test_list)\n",
    "\n",
    "te_labels = pd.DataFrame({\"Video\":test_list,\n",
    "                         \"partition\":test_label})\n",
    "label_df = pd.concat([tr_labels,te_labels])\n",
    "label_df[\"Video\"] = label_df[\"Video\"].str.replace(\"MSAD_normal_\", \"\", regex=False)\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3e16f7e-bd3e-4ac5-a74f-7e871eb1f3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partition</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Video\n",
       "partition       \n",
       "Test         240\n",
       "Train        480"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.groupby(\"partition\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66290c63-c6bf-4890-8af0-c343ba1170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left= metadata, right = label_df , on= \"Video\",how= \"left\")\n",
    "df_train =  df[df[\"partition\"] == \"Train\"].drop(columns= \"partition\")\n",
    "df_test =  df[df[\"partition\"] == \"Test\"].drop(columns= \"partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4871a32e-3184-47e9-a3d9-2fd74660bd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57190"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8375e646-5a7a-495e-9eb3-1ce828fa3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, labels_df, transform=None, clip_length=16, step_size=8):\n",
    "        \"\"\"\n",
    "        labels_df: DataFrame with columns [\"Video\", \"Frame\", \"Frames_path\", \"Anomaly Type\", \"Anomaly\"]\n",
    "        transform: Image transformations (for resizing, normalizing, etc.)\n",
    "        clip_length: Number of frames per sample (default 16 for ResNet3D)\n",
    "        step_size: How far the window moves per sample (default 8 frames)\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df.copy()  # Prevent modifying the original DataFrame\n",
    "        self.transform = transform\n",
    "        self.clip_length = clip_length\n",
    "        self.step_size = step_size\n",
    "\n",
    "        # Ensure data is sorted by video and frame number\n",
    "        self.labels_df.sort_values(by=[\"Video\", \"Frame\"], inplace=True)\n",
    "\n",
    "        # Group frames by video\n",
    "        self.video_groups = self.labels_df.groupby(\"Video\")\n",
    "\n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels_df[\"Encoded_Label\"] = self.label_encoder.fit_transform(self.labels_df[\"Anomaly\"])\n",
    "\n",
    "        # Store unique videos\n",
    "        self.video_list = list(self.video_groups.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        total_clips = 0\n",
    "        for video_name in self.video_list:\n",
    "            num_frames = len(self.video_groups.get_group(video_name))\n",
    "            num_clips = max(0, (num_frames - self.clip_length) // self.step_size + 1)  # Fix possible overestimation\n",
    "            total_clips += num_clips\n",
    "        return total_clips\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine video & clip index\n",
    "        total_clips = 0\n",
    "        for video_name in self.video_list:\n",
    "            video_frames = self.video_groups.get_group(video_name)\n",
    "            num_frames = len(video_frames)\n",
    "            num_clips = max(0, (num_frames - self.clip_length) // self.step_size + 1)\n",
    "\n",
    "            if idx < total_clips + num_clips:\n",
    "                clip_idx = idx - total_clips\n",
    "                break\n",
    "\n",
    "            total_clips += num_clips\n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx} is out of range for dataset of length {self.__len__()}\")\n",
    "\n",
    "        # Get frames & labels\n",
    "        frame_paths = video_frames[\"Frames_path\"].tolist()\n",
    "        frame_labels = video_frames[\"Encoded_Label\"].tolist()\n",
    "\n",
    "        start_idx = clip_idx * self.step_size\n",
    "\n",
    "        selected_frames = frame_paths[start_idx:start_idx + self.clip_length]\n",
    "        selected_labels = frame_labels[start_idx:start_idx + self.clip_length]\n",
    "\n",
    "        if not selected_frames:  # Prevent infinite loop\n",
    "            raise ValueError(f\"No frames found for clip at index {idx}\")\n",
    "\n",
    "        # Pad if needed\n",
    "        while len(selected_frames) < self.clip_length:\n",
    "            selected_frames.append(selected_frames[-1])\n",
    "\n",
    "        while len(selected_labels) < self.clip_length:\n",
    "            selected_labels.append(selected_labels[-1])\n",
    "\n",
    "        # Load frames\n",
    "        frames = []\n",
    "        for frame_path in selected_frames:\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is None:\n",
    "                raise ValueError(f\"Error reading image: {frame_path}\")\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            else:\n",
    "                frame = torch.from_numpy(frame).float()\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        frames = torch.stack(frames)  # Shape: (16, 3, H, W)\n",
    "\n",
    "        # Assign majority label\n",
    "        clip_label = torch.tensor(selected_labels).mode()[0]  # Get most frequent label\n",
    "\n",
    "        return frames.permute(3, 0, 1, 2), torch.tensor(clip_label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3773ae5a-cc47-4ddb-9d5f-28d6b91aa876",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts NumPy array to Tensor\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7c47c54-7adc-4b81-96b5-e0a168ef68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 6505\n",
      "testing dataset size: 3290\n"
     ]
    }
   ],
   "source": [
    "training_dataset = FrameDataset(labels_df = df_train)\n",
    "testing_dataset = FrameDataset(labels_df = df_test)\n",
    "print(\"Training dataset size: {}\\ntesting dataset size: {}\".format(len(training_dataset),len(testing_dataset)))\n",
    "num_workers = min(6, os.cpu_count() - 1)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=256, shuffle=False, num_workers=0)##, pin_memory=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=256, shuffle=False, num_workers=0)#, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15aa48-f413-4d5e-80db-3507224aeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anomalies = 0\n",
    "total_samples = 0\n",
    "\n",
    "for _, labels in training_dataloader:\n",
    "    total_anomalies += torch.sum(labels).item()\n",
    "    total_samples += len(labels)\n",
    "\n",
    "print(f\"Total anomalies in dataloader: {total_anomalies}/{total_samples} ({(total_anomalies/total_samples)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b748b-1347-43d9-8a0c-aae781ef8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anomalies = 0\n",
    "total_samples = 0\n",
    "\n",
    "for _, labels in testing_dataloader:\n",
    "    total_anomalies += torch.sum(labels).item()\n",
    "    total_samples += len(labels)\n",
    "\n",
    "print(f\"Total anomalies in dataloader: {total_anomalies}/{total_samples} ({(total_anomalies/total_samples)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc867c3-7cca-4015-af9c-a48cbbb7261e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_binary \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mr3d_18(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m model_binary \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(model_binary\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m      3\u001b[0m              nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[0;32m      4\u001b[0m              nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m      5\u001b[0m              nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m      6\u001b[0m              nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      7\u001b[0m              nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m1\u001b[39m),)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model_binary = models.video.r3d_18(pretrained=True)\n",
    "model_binary = nn.Sequential(*list(model_binary.children())[:-1], \n",
    "             nn.Flatten(),\n",
    "             nn.Linear(512,256),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(0.5),\n",
    "             nn.Linear(256,1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c472a1d-079e-48df-a977-35a7a1e191d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftF1Loss(torch.nn.Module):\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super(SoftF1Loss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        y_pred: Model outputs (logits or probabilities after sigmoid)\n",
    "        y_true: Ground truth labels (binary: 0 or 1)\n",
    "        \"\"\"\n",
    "        y_pred = torch.sigmoid(y_pred)  # Ensure predictions are between 0 and 1\n",
    "        \n",
    "        tp = torch.sum(y_true * y_pred)  # True Positives\n",
    "        fp = torch.sum((1 - y_true) * y_pred)  # False Positives\n",
    "        fn = torch.sum(y_true * (1 - y_pred))  # False Negatives\n",
    "\n",
    "        f1 = (2 * tp + self.epsilon) / (2 * tp + fp + fn + self.epsilon)\n",
    "        return 1 - f1  # Minimize (1 - F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2d22-00f3-4bd3-9eb5-9dd43b7fa982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training pass:   0%|                                                                                                    | 0/26 [00:00<?, ?it/s]C:\\Users\\Keelan.Butler\\AppData\\Local\\Temp\\ipykernel_16092\\503329530.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return frames.permute(3, 0, 1, 2), torch.tensor(clip_label, dtype=torch.long)\n",
      "Training pass:  65%|████████████████████████████████████████████████████████▏                             | 17/26 [2:56:36<1:51:31, 743.54s/it]"
     ]
    }
   ],
   "source": [
    "# Training Hyperparameters\n",
    "\n",
    "epochs = 25\n",
    "#pos_weight = torch.tensor([5.0]).to(device)\n",
    "losses = np.zeros((2, epochs))\n",
    "\n",
    "\n",
    "optimiser = torch.optim.Adam(model_binary.parameters(), lr=1e-4, weight_decay=1e-4) # I've changed this back from AdamW as the data appears to be less imbalanced\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()# BCEWithLogitsLoss\n",
    "\n",
    "#loss_function = SoftF1Loss()\n",
    "\n",
    "# Use GPU if available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_binary.to(device)\n",
    "best_loss = np.inf\n",
    "\n",
    "\n",
    "threshhold = 0.5 \n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model_binary.train()\n",
    "\n",
    "    for frames, labels in tqdm(training_dataloader,desc = \"Training pass\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)  # Move data to GPU if available\n",
    "        pred = model_binary(frames)  # Forward pass\n",
    "        loss = loss_function(pred.squeeze(), labels.float())\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Store training loss\n",
    "    losses[0, epoch] = epoch_loss / len(training_dataloader)\n",
    "\n",
    "    # Validation Loop\n",
    "    model_binary.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_frames, test_labels in tqdm(testing_dataloader,desc = \"Cycling Testing Dataloader\"):\n",
    "            test_frames, test_labels = test_frames.to(device), test_labels.to(device)  # Move to GPU\n",
    "\n",
    "            test_preds = model_binary(test_frames)\n",
    "            t_loss = loss_function(test_preds.squeeze(), test_labels.float())\n",
    "\n",
    "            test_loss += t_loss.item()\n",
    "            \n",
    "    losses[1, epoch] = test_loss / len(testing_dataloader)\n",
    "    if best_loss > losses[1, epoch]:\n",
    "        best_loss = losses[1, epoch] \n",
    "        print(\"Saving Optimal model: {} epoch\".format(epoch + 1))\n",
    "        torch.save(model_binary.state_dict(),os.path.join(\"Best_Models\",\"E2E_3DCNN.pt\"))\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {losses[0,epoch]:.4f}, Test Loss: {losses[1,epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce6961-562e-4ff3-bbce-327583213478",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0], label = 'Training')\n",
    "plt.plot(losses[1], label = 'Testing')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"R3D CNN Training Binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87719967-469f-423b-9cbb-1ae6efa5d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    }
   ],
   "source": [
    "model_binary.load_state_dict(torch.load(os.path.join(\"Best_Models\", \"E2E_3DCNN.pt\")))\n",
    "print(\"Best model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53820b2-bcdd-496a-b73e-2d0801af1fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(all_labels), np\u001b[38;5;241m.\u001b[39marray(all_preds)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[43mdevice\u001b[49m)  \u001b[38;5;66;03m# Change to \"cuda\" if using GPU\u001b[39;00m\n\u001b[0;32m     22\u001b[0m true_labels, pred_labels \u001b[38;5;241m=\u001b[39m get_predictions(model_binary, testing_dataloader, device)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal test samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(true_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Expected: 29270)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for frames, labels in dataloader:\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(frames)  # Forward pass\n",
    "           # _, preds = torch.max(outputs, 1)  # Get predicted class MULTICLASS APPROACH\n",
    "            #preds = (outputs >= 0.5).float() # Binalry class \n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(device)  # Change to \"cuda\" if using GPU\n",
    "true_labels, pred_labels = get_predictions(model_binary, testing_dataloader, device)\n",
    "print(f\"Total test samples: {len(true_labels)} (Expected: 29270)\")\n",
    "print(f\"Total predictions: {len(pred_labels)} (Expected: 29270)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58210513-3410-46bc-b2a2-a95175cd0941",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtesting_dataloader\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(testing_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(testing_dataloader)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mtesting_dataloader\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testing_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Batch size: {testing_dataloader.batch_size}\")\n",
    "print(f\"Total batches: {len(testing_dataloader)}\")\n",
    "print(f\"Total samples processed: {len(testing_dataloader) * testing_dataloader.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27708ba0-019b-4e20-a9c3-bd5874fbc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(true_labels, pred_labels)\n",
    "sns.heatmap(cm , annot = True)\n",
    "plt.title(\"Confusion Matrix of 3D ResNet-18 Classifier\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc432fb-9697-45ff-8d0f-41cbccb7c391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
